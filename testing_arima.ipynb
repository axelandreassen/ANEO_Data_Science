{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 13:58:48.009700: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import KFold\n",
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyperparameters<h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Creating DataFrames<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_prices = pd.read_csv(\"spotpriser.csv\").set_index(\"time\")\n",
    "consumption = pd.read_csv(\"consumption_temp_with_flags.csv\").set_index(\"time\")\n",
    "\n",
    "oslo_consumption = consumption[consumption[\"location\"] == \"oslo\"]\n",
    "oslo_consumption = oslo_consumption.merge(spot_prices[\"oslo\"], left_index=True, right_index=True, how=\"left\")\n",
    "oslo_consumption=oslo_consumption.drop(['location'],axis=1).rename(columns={'oslo':'spot_price'})\n",
    "\n",
    "stavanger_consumption = consumption[consumption[\"location\"] == \"stavanger\"]\n",
    "stavanger_consumption = stavanger_consumption.merge(spot_prices[\"stavanger\"], left_index=True, right_index=True, how=\"left\")\n",
    "stavanger_consumption=stavanger_consumption.drop(['location'],axis=1).rename(columns={'stavanger':'spot_price'})\n",
    "\n",
    "trondheim_consumption = consumption[consumption[\"location\"] == \"trondheim\"]\n",
    "trondheim_consumption = trondheim_consumption.merge(spot_prices[\"trondheim\"], left_index=True, right_index=True, how=\"left\")\n",
    "trondheim_consumption=trondheim_consumption.drop(['location'],axis=1).rename(columns={'trondheim':'spot_price'})\n",
    "\n",
    "tromso_consumption = consumption[consumption[\"location\"] == \"tromsø\"]\n",
    "tromso_consumption = tromso_consumption.merge(spot_prices[\"tromsø\"], left_index=True, right_index=True, how=\"left\")\n",
    "tromso_consumption=tromso_consumption.drop(['location'],axis=1).rename(columns={'tromsø':'spot_price'})\n",
    "\n",
    "bergen_consumption = consumption[consumption[\"location\"] == \"bergen\"]\n",
    "bergen_consumption = bergen_consumption.merge(spot_prices[\"bergen\"], left_index=True, right_index=True, how=\"left\")\n",
    "bergen_consumption=bergen_consumption.drop(['location'],axis=1).rename(columns={'bergen':'spot_price'})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature engineering<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMPTION_DFS = [oslo_consumption,stavanger_consumption,trondheim_consumption,tromso_consumption,bergen_consumption]\n",
    "\n",
    "cols_to_standard_normal = [\"temperature\",\"delta_temp\"]\n",
    "cols_to_normalize_01 = [\"consumption\",\"spot_price\"]\n",
    "\n",
    "for df in CONSUMPTION_DFS:\n",
    "    df[\"delta_temp\"] = df[\"temperature\"]-df[\"temperature\"].shift(1)\n",
    "    df[\"is_holiday\"] = df[\"is_holiday\"].astype(int)\n",
    "    df[\"is_weekend\"] = df[\"is_weekend\"].astype(int)\n",
    "\n",
    "    for col in cols_to_standard_normal:\n",
    "        mu, sig = df[col].mean(), df[col].std()\n",
    "        df[col] = (df[col]-mu)/sig\n",
    "    for col in cols_to_normalize_01:\n",
    "        df[col] = (df[col]-df[col].min())/(df[col].max()-df[col].min())\n",
    "    df = df.iloc[1:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Splitting datasets<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "oslo_with_price = oslo_consumption.dropna()\n",
    "oslo_no_price = pd.concat([oslo_consumption,oslo_with_price]).drop_duplicates(keep=False).drop(columns=[\"spot_price\"])\n",
    "\n",
    "stavanger_with_price = stavanger_consumption.dropna()\n",
    "stavanger_no_price = pd.concat([stavanger_consumption,stavanger_with_price]).drop_duplicates(keep=False).drop(columns=[\"spot_price\"])\n",
    "\n",
    "trondheim_with_price = trondheim_consumption.dropna()\n",
    "trondheim_no_price = pd.concat([trondheim_consumption,trondheim_with_price]).drop_duplicates(keep=False).drop(columns=[\"spot_price\"])\n",
    "\n",
    "tromso_with_price = tromso_consumption.dropna()\n",
    "tromso_no_price = pd.concat([tromso_consumption,tromso_with_price]).drop_duplicates(keep=False).drop(columns=[\"spot_price\"])\n",
    "\n",
    "bergen_with_price = bergen_consumption.dropna()\n",
    "bergen_no_price = pd.concat([bergen_consumption,bergen_with_price]).drop_duplicates(keep=False).drop(columns=[\"spot_price\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/js/9hpr3tt17xd6ttv9xy3zrhw80000gn/T/ipykernel_9853/1307957567.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ALL_DFS[i].loc[:,\"hour\"] = ALL_DFS[i].index.hour\n",
      "/var/folders/js/9hpr3tt17xd6ttv9xy3zrhw80000gn/T/ipykernel_9853/1307957567.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ALL_DFS[i].loc[:,\"hour\"] = ALL_DFS[i].index.hour\n",
      "/var/folders/js/9hpr3tt17xd6ttv9xy3zrhw80000gn/T/ipykernel_9853/1307957567.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ALL_DFS[i].loc[:,\"hour\"] = ALL_DFS[i].index.hour\n",
      "/var/folders/js/9hpr3tt17xd6ttv9xy3zrhw80000gn/T/ipykernel_9853/1307957567.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ALL_DFS[i].loc[:,\"hour\"] = ALL_DFS[i].index.hour\n",
      "/var/folders/js/9hpr3tt17xd6ttv9xy3zrhw80000gn/T/ipykernel_9853/1307957567.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ALL_DFS[i].loc[:,\"hour\"] = ALL_DFS[i].index.hour\n"
     ]
    }
   ],
   "source": [
    "ALL_DFS = [oslo_with_price,oslo_no_price,stavanger_with_price,stavanger_no_price,trondheim_with_price,trondheim_no_price,tromso_with_price,tromso_no_price,bergen_with_price,bergen_no_price]\n",
    "\n",
    "for i in range(len(ALL_DFS)):\n",
    "    ALL_DFS[i].index = pd.to_datetime(ALL_DFS[i].index)\n",
    "    ALL_DFS[i].loc[:,\"hour\"] = ALL_DFS[i].index.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Making training data<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erikmjaanes/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/erikmjaanes/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/erikmjaanes/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    }
   ],
   "source": [
    "Y = oslo_with_price[\"consumption\"]\n",
    "X = oslo_with_price.drop(\"consumption\", axis = 1)\n",
    "X = X - X-X.shift(24)\n",
    "X.dropna(inplace=True, axis = 0)\n",
    "Y = Y.iloc[24:]\n",
    "\n",
    "predictions = {}\n",
    "for column in df.columns:\n",
    "    # Split data into train and test\n",
    "    train, test = train_test_split(df[column], train_size=0.8)\n",
    "\n",
    "    # Fit an auto-ARIMA model\n",
    "    model = pm.auto_arima(y=Y,X=X, seasonal=True, m=24, start_P=120, start_Q=120, start_p = 120, start_q=120, max_p=122,max_q=122,max_P=122,max_Q=122, information_criterion='bic') # m is the seasonal order\n",
    "\n",
    "    # Make predictions\n",
    "    forecast = model.predict(n_periods=len(test))\n",
    "\n",
    "    # Store predictions\n",
    "    predictions[column] = forecast\n",
    "\n",
    "    # Plot the results (optional)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train.index, train, label='Train')\n",
    "    plt.plot(test.index, test, label='Test')\n",
    "    plt.plot(test.index, forecast, label='Forecast')\n",
    "    plt.title(f'Forecast vs Actuals for {column}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "for column in df.columns:\n",
    "    test = df[column][-len(predictions[column]):]  # Get the corresponding test set\n",
    "    rmse = sqrt(mean_squared_error(test, predictions[column]))\n",
    "    print(f'RMSE for {column}: {rmse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
